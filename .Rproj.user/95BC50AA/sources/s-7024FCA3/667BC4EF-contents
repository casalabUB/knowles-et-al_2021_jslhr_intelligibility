# Tidy VAS
# Consider examining CON/SITs together...
library(plyr)
library(tidyverse)
#library("tidylog", warn.conflicts = FALSE)
library(cowplot)
library(lme4)
library(lmerTest)
library(emmeans)
library(rms)
library(psych) # for ICC
library(ggplot2)
library(thear)
library(ggridges)

#cat("reading in data")
# Load data----
#source("../scripts/RFunctions.R")
path = "../data/intelligibility/"
path_acoustics = "../data/acoustics/"
filter <- dplyr::filter
select <- dplyr::select

# _CON----
conv <- read_csv(paste0(path,"/conv/2019-03-18/vas_responses_conv.csv")) %>%
     radi_filename_prep() %>%
     dplyr::filter(!is.na(participant)) %>%# get rid of headers
     mutate(percentage = as.numeric(percentage),
            order = as.numeric(order),
            session = as.numeric(session),
            listener_block = listener,
            listener = factor(str_sub(listener, 1, 1)),
            block = factor(str_sub(listener, 3, 6))) %>% # NB block isn't necessary for conv, only sits
     mutate(participant = revalue(participant, replace = c( # Fix 509, 303
          "5092" = "509",
          "3032" = "303"))) %>%
        mutate(id = paste(condition,participant,item,sep="_"))

# _SITs----
sits <- read_csv(paste0(path,"/sits/2019-03-27/vas_responses.csv")) %>%
     radi_filename_prep() %>%
     # get rid of headers
     dplyr::filter(!is.na(participant)) %>%
     mutate(percentage = as.numeric(percentage),
            order = as.numeric(order),
            session = as.numeric(session),
            listener_block = listener,
            block = factor(str_sub(listener, 3, 6)),
            listener = factor(str_sub(listener, 1, 1))) %>% # NB block isn't necessary for conv, only sits
     mutate(participant = revalue(participant, replace = c( # Fix 509, 303
          "5092" = "509",
          "3032" = "303"))) %>%
        mutate(id = paste(condition,participant,item,sep="_"))

conv_orig <- conv
sits_orig <- sits

## Reliability----
library(dplyr)
remove_duplicates(conv)
remove_duplicates(sits)
# conv now contains only unique values for each listener
conv <- conv_unique %>% mutate(task = "conv", block = "conv")
sits <- sits_unique %>% mutate(task = "sits")
vas <- rbind(conv, sits) %>% 
     mutate(block = factor(block),
            task = factor(task))
# XTODO: fix (in raw data plz) where listener 4 didn't properly have block identified (should be sitsD; currently block is empty)
# For now, fix here:
#vas[vas$block=="" & vas$listener=="4",]$block <- "sitD"
vas$block <- factor(vas$block, levels = c("conv", "sitA", "sitB", "sitC", "sitD"))

# Explore
# CON: removed 288 files; 48 per listener
(nrow(conv_orig) - nrow(conv_unique))/length(unique(conv_orig$listener)) 
nrow(conv_duplicate) # good
# SIT: removed 2176 files, 363/listener
(nrow(sits_orig) - nrow(sits_unique))/length(unique(sits_orig$listener)) 
# NB: listeners 1 and 2 had more reliability files; not sure why this is #OH I KNOW MORE RELIABILITY OOPS
#sits_l01_tmp <- subset(sits_unique, listener=="1")


# Extract exemplars from unique that have a match in duplicate
# .x refers to conv_duplicate
conv_joined_dup <- left_join(conv_duplicate, conv_unique, by = c("filename", "listener")) %>%
     select(filename, listener, order.x, percentage.x, order.y, percentage.y)
names(conv_joined_dup) <- c("filename", "listener", "order_1", "percentage_1", "order_2", "percentage_2")

sits_joined_dup <- left_join(sits_duplicate, sits_unique, by = c("filename", "listener")) %>%
     select(filename, listener, order.x, percentage.x, order.y, percentage.y)
names(sits_joined_dup) <- c("filename", "listener", "order_1", "percentage_1", "order_2", "percentage_2")

#View(sits_joined_dup)


# _ICCs:----
#    Average intra-rater reliability: ICC3k
#    Average inter-rater reliability: ICC2k


# _Inter-rater----

conv_inter <- conv_joined_dup %>%
     select(percentage_1, percentage_2) %>%
     ICC()
conv_inter <- conv_inter$results %>%
     filter(type=="ICC2k" | type=="ICC3k")

sits_inter <- sits_joined_dup %>%
     select(percentage_1, percentage_2) %>%
     ICC()
sits_inter <- sits_inter$results %>%
     filter(type=="ICC2k" | type=="ICC3k") # pvalue is 0 but maybe that's ok once it becomes so small

# Conv and Sits together
vas_joined_dup <- rbind(conv_joined_dup, sits_joined_dup)
vas_inter <- vas_joined_dup %>%
     select(percentage_1, percentage_2) %>%
     ICC()

# _Intra-rater ----
# __SITs----
sits_L01 <- sits_joined_dup %>%
        filter(listener=="1") %>%
        select(percentage_1, percentage_2) %>%
        ICC()
sits_L01 <- sits_L01$results %>% mutate(listener=1)
sits_L02 <- sits_joined_dup %>%
        filter(listener=="2") %>%
        select(percentage_1, percentage_2) %>%
        ICC()
sits_L02 <- sits_L02$results %>% mutate(listener=2)
sits_L03 <- sits_joined_dup %>%
        filter(listener=="3") %>%
        select(percentage_1, percentage_2) %>%
        ICC()
sits_L03 <- sits_L03$results %>% mutate(listener=3)
sits_L04 <- sits_joined_dup %>% # THIS ONE DOESNT CONVERGE with LMER SO USED AOV
        filter(listener=="4") %>%
        select(percentage_1, percentage_2) %>%
        as.data.frame() %>%
        ICC(lmer=FALSE)
sits_L04 <- sits_L04$results %>% mutate(listener=4)
sits_L05 <- sits_joined_dup %>%
        filter(listener=="5") %>%
        select(percentage_1, percentage_2) %>%
        ICC()
sits_L05 <- sits_L05$results %>% mutate(listener=5)
sits_L06 <- sits_joined_dup %>%
        filter(listener=="6") %>%
        select(percentage_1, percentage_2) %>%
        as.data.frame() %>%
        ICC(lmer=FALSE)
sits_L06 <- sits_L06$results %>% mutate(listener=6)


# __Conv ----
# Wait to finish this until I decide wither to join conv and sits
# Single listener
conv_L01 <- conv_joined_dup %>% # simgular fit with lmer so using aov
     filter(listener=="1") %>%
     select(percentage_1, percentage_2) %>%
        as.data.frame() %>%
        ICC(lmer=FALSE)
conv_L01 <- conv_L01$results %>% mutate(listener=1)
conv_L02 <- conv_joined_dup %>%
     filter(listener=="2") %>%
     select(percentage_1, percentage_2) %>%
     ICC()
conv_L02 <- conv_L02$results %>% mutate(listener=2)
conv_L03 <- conv_joined_dup %>%
     filter(listener=="3") %>%
     select(percentage_1, percentage_2) %>%
     ICC()
conv_L03 <- conv_L03$results %>% mutate(listener=3)
conv_L04 <- conv_joined_dup %>% # failed to converge with lmer so using aov
     filter(listener=="4") %>%
     select(percentage_1, percentage_2) %>%
        as.data.frame() %>%
        ICC(lmer=FALSE)
conv_L04 <- conv_L04$results %>% mutate(listener=4)
conv_L05 <- conv_joined_dup %>%
     filter(listener=="5") %>%
     select(percentage_1, percentage_2) %>%
     ICC()
conv_L05 <- conv_L05$results %>% mutate(listener=5)
conv_L06 <- conv_joined_dup %>%
     filter(listener=="6") %>%
     select(percentage_1, percentage_2) %>%
     ICC()
conv_L06 <- conv_L06$results %>% mutate(listener=6)


# Average intRA-rater reliability: ICC3k
sits_intra <- rbind(sits_L01,sits_L02,sits_L03,sits_L04,sits_L05,sits_L06) %>%
        filter(type=="ICC3k") %>%
        mutate(listener = factor(listener)) %>%
        select(listener, everything()) %>%
        mutate(p = if_else(p <= 0.001, "<0.001", as.character(p)))
sits_intra_mean <- mean(sits_intra$ICC)
sits_intra_min <- min(sits_intra$ICC)
sits_intra_max <- max(sits_intra$ICC)

conv_intra <- rbind(conv_L01,conv_L02,conv_L03,conv_L04,conv_L05,conv_L06) %>%
     filter(type=="ICC3k") %>%
        mutate(listener = factor(listener)) %>%
        select(listener, everything()) %>%
        mutate(p = if_else(p <= 0.001, "<0.001", as.character(p)))
conv_intra_mean <- mean(conv_intra$ICC)
conv_intra_min <- min(conv_intra$ICC)
conv_intra_max <- max(conv_intra$ICC)



# SITS duration ----
sits_dur <- read_csv(paste0(path_acoustics,"radi_sits_duration.csv")) %>%
        mutate(id = str_sub(filename,1,nchar(filename)-2))
# Merge with SITs
# Create nwords column
sits <- merge(sits,sits_dur, by = "id", all = TRUE)  %>%
        mutate(nwords = item) %>%
        mutate(nwords = recode(nwords, 
                               "53" = "5",
                               "54" = "6",
                               "55" = "7",
                               "56" = "8",
                               "57" = "9",
                               "58" = "10")) %>%
        mutate(nwords = as.numeric(as.character(nwords)))

## TODO: Speech rate
#    - for now: just do rate condition?


# Calculate rates for SITS ----
# NB I fucked up and OC208 and DBS512 were not played for listeners for the sits (but were for conv)
habit_df <- thear::calculate_habitual_mean(sits)
sits <- thear::calculate_radi_rates(sits)

# Check
library(ggridges)
sits %>% ggplot(aes(wpm)) + geom_histogram()+facet_wrap(~group)
sits %>% ggplot(aes(x=prop_wpm, y=prop_wpm_5))+geom_density_ridges()+facet_wrap(~group)

set.seed(1)
sits %>% tjmisc::sample_n_of(participant, size=6) %>% 
        ggplot(aes(x=wpm, y=prop_wpm_5))+geom_density_ridges()+facet_wrap(~participant)




## Summarize----
## Goal: 
#    - Each file should have response averaged over all listeners

## _VAS, rate ----
# Aggregate over listener, participant

vas_lp <- vas %>%
        group_by(task, item, listener, participant, group, rate) %>%
        summarise(percentage = mean(percentage, na.rm = TRUE))
detach(package:dplyr)
vas_p <- summarySE(vas_lp, measurevar = "percentage", groupvars = c("task","item", "participant", "group", "rate"))
vas_grt <- summarySE(vas_lp, measurevar = "percentage", groupvars = c("task", "group", "rate"))
vas_gr <- summarySE(vas_lp, measurevar = "percentage", groupvars = c("group", "rate"))
vas_it <- summarySE(vas_lp, measurevar = "percentage", groupvars = c("task", "item", "rate"))
library(dplyr)



# __Contrasts ----
vas_p <- vas_p %>% filter(!is.na(group))
contrasts(vas_p$group) <- matrix(c(3/4, -1/4, -1/4, -1/4,
                                   0, 2/3, -1/3, -1/3,
                                   0, 0, 1/2, -1/2),
                                 ncol = 3)
colnames(contrasts(vas_p$group)) <- c("YC vs rest", "OC vs rest", "PD vs DBS")
solve(cbind(1,contrasts(vas_p$group)))


# Refactor rate so that level 1 is habitual, then code as treatment so every level is compared to habitual
# First make original rate column so that it's properly ordered slow to fast
# vas_p <- vas_p %>%
#         mutate(rate = str_to_upper(rate))
vas_p$rate_ordered <- vas_p$rate
vas_p$rate <- factor(vas_p$rate, 
                     levels =c("H1", "S4", "S3", "S2", "F2", "F3", "F4"))
contrasts(vas_p$rate) <- contr.treatment(7)
solve(cbind(1,contrasts(vas_p$rate)))
colnames(contrasts(vas_p$rate)) <- c("S4vH1","S3vH1","S2vH1","F2vH1","F3vH1","F4vH1")

# Task: sum coding
contrasts(vas_p$task) <- contr.sum(2)
contrasts(vas_p$task)

# Item: helmert if used as fixed eff
#levels(vas_p$item) <- c("53","54","55","56","57","58","63") # OLD
levels(vas_p$item) <- c("53","56","54","57","55","58","63") # CORRECT
vas_p$complexity <- vas_p$item
contrasts(vas_p$complexity) <- contr.helmert(7)
solve(cbind(1,contrasts(vas_p$complexity)))
#colnames(contrasts(vas_p$complexity)) <- c("53v54","53-54v55","53-55v56","53-56v57","53-57v58", "53-58v63")
colnames(contrasts(vas_p$complexity)) <- c("5v6","5-6v7","5-7v8","5-8v9","5-9v10", "Task")


# Item: make item_new so that each item across conditions is unique
vas_p$item_new <- factor(paste0(vas_p$item,"_",vas_p$rate))
# 53 - 58: sentences increase in length; 63: conversation
levels(vas_p$item_new)
# Rescale item, participant
# NB: rescaling actually makes 0 difference in model fit or coefficients, so no need to do this anymore
vas_p <- vas_p %>%
        transform(participant_rescaled = arm::rescale(participant),
                  item_new_rescaled = arm::rescale(item_new))






# _SITS, ratePrOP ----
# For sits, need to also do all of this with rateProp
# item_new not use in models here because complexity ~= item, but needed to summarize sits otherwise data is collapsed
sits_lp <- sits %>%
        mutate(item_new = paste(item, rate, sep="_")) %>%
        group_by(task, item, item_new, listener, participant, group, prop_wpm_5) %>%
        summarise(percentage = mean(percentage, na.rm = TRUE))

detach(package:dplyr)
sits_p <- summarySE(sits_lp, measurevar = "percentage", groupvars = c("task","item", "item_new","participant", "group", "prop_wpm_5"))
sits_grt <- summarySE(sits_lp, measurevar = "percentage", groupvars = c("task", "group", "prop_wpm_5"))
sits_gr <- summarySE(sits_lp, measurevar = "percentage", groupvars = c("group", "prop_wpm_5"))
sits_it <- summarySE(sits_lp, measurevar = "percentage", groupvars = c("task","item", "item_new", "prop_wpm_5"))
library(dplyr)

# __Contrasts ----
contrasts(sits_p$group) <- matrix(c(3/4, -1/4, -1/4, -1/4,
                                   0, 2/3, -1/3, -1/3,
                                   0, 0, 1/2, -1/2),
                                 ncol = 3)
colnames(contrasts(sits_p$group)) <- c("YC vs rest", "OC vs rest", "PD vs DBS")
solve(cbind(1,contrasts(sits_p$group)))


# Refactor rate so that level 1 is habitual, then code as treatment so every level is compared to habitual
# First make original rate column so that it's properly ordered slow to fast
# sits_p <- sits_p %>%
#         mutate(rate = str_to_upper(rate))
sits_p$prop_wpm_5 <- factor(sits_p$prop_wpm_5, 
                     levels =c("H1", "S3", "S2", "F2", "F3"))
contrasts(sits_p$prop_wpm_5) <- contr.treatment(5)
solve(cbind(1,contrasts(sits_p$prop_wpm_5)))
colnames(contrasts(sits_p$prop_wpm_5)) <- c("S3vH1","S2vH1","F2vH1","F3vH1")

# Task: sum coding
#contrasts(sits_p$task) <- contr.sum(2)
#contrasts(sits_p$task)

# Item: helmert if used as fixed eff
# Omit item 63 for just sits
# NB: item_new excluded in these models because it overlaps with complexity
#levels(sits_p$item) <- c("53","54","55","56","57","58") # OLD
levels(sits_p$item) <- c("53","56","54","57","55","58") # CORRECT ORDERING
sits_p$complexity <- sits_p$item
# Recode sentence length
# sits_p <- sits_p %>% # This is wrong; item n not perfectly aligned with sentence lenght. See update below
#         mutate(sentence_length = complexity) %>%
#         mutate(sentence_length = recode(sentence_length,
#                                                        "53" = "5",
#                                                        "54" = "7",
#                                                        "55" = "9",
#                                                        "56" = "6",
#                                                        "57" = "8",
#                                                        "58" = "10"))# same as complexity but reverse helmert coding to make effect direction interpretable
# Contrasts for complexity (helmert)
# Multiplied contrasts by -1 so that direction is consistent with group and rate
# i.e., a negative contrast = the contrast on the left was better
contrasts(sits_p$complexity) <- contr.helmert(6)*-1
colnames(contrasts(sits_p$complexity)) <- c("5v6","5-6v7","5-7v8","5-8v9","5-9v10")
solve(cbind(1,contrasts(sits_p$complexity)))



# Consider using nwords instead of complexity for sits model
# UPDATE: nwords is now correct
sits_p <- sits_p %>%
        mutate(nwords = item) %>%
        mutate(nwords = recode(nwords, 
                               "53" = "5",
                               "56" = "6",
                               "54" = "7",
                               "57" = "8",
                               "55" = "9",
                               "58" = "10")) %>%
        mutate(nwords = as.numeric(as.character(nwords)))

sits_p$sentence_length <- sits_p$nwords


# Item_new: this is done up above
# Used rate since items are nested by rate
#sits_p$item_new <- factor(paste0(sits_p$item,"_",sits_p$condition))
# 53 - 58: sentences increase in length; 63: conversation
# Rescale item, participant
# NB: rescaling actually makes 0 difference in model fit or coefficients, so no need to do this anymore
sits_p <- sits_p %>%
        transform(participant_rescaled = arm::rescale(participant)) # no item_new here

# _CONV, rate ----
# __Contrasts ----
conv_p <- vas_p %>% filter(task=="conv")
contrasts(conv_p$group) <- matrix(c(3/4, -1/4, -1/4, -1/4,
                                   0, 2/3, -1/3, -1/3,
                                   0, 0, 1/2, -1/2),
                                 ncol = 3)
colnames(contrasts(conv_p$group)) <- c("YC vs rest", "OC vs rest", "PD vs DBS")
solve(cbind(1,contrasts(conv_p$group)))

conv_p$rate_ordered <- conv_p$rate
conv_p$rate <- factor(conv_p$rate, 
                     levels =c("H1", "S4", "S3", "S2", "F2", "F3", "F4"))
contrasts(conv_p$rate) <- contr.treatment(7)
solve(cbind(1,contrasts(conv_p$rate)))
colnames(contrasts(conv_p$rate)) <- c("S4vH1","S3vH1","S2vH1","F2vH1","F3vH1","F4vH1")


# _Slopes ----

contrasts(vas_p$group)
model.matrix(~group, vas_p) %>% head()
vas_p$YCvRest <- model.matrix(~group, vas_p)[,2]
vas_p$OCvRest <- model.matrix(~group, vas_p)[,3]
vas_p$PDvDBS <-  model.matrix(~group, vas_p)[,4]

model.matrix(~group, sits_p) %>% head()
sits_p$YCvRest <- model.matrix(~group, sits_p)[,2]
sits_p$OCvRest <- model.matrix(~group, sits_p)[,3]
sits_p$PDvDBS <-  model.matrix(~group, sits_p)[,4]

model.matrix(~group, conv_p) %>% head()
conv_p$YCvRest <- model.matrix(~group, conv_p)[,2]
conv_p$OCvRest <- model.matrix(~group, conv_p)[,3]
conv_p$PDvDBS <-  model.matrix(~group, conv_p)[,4]

# VAS: rate
model.matrix(~rate, vas_p) %>% head()
vas_p$S4vH1_rate <- model.matrix(~rate, vas_p)[,2]
vas_p$S3vH1_rate <- model.matrix(~rate, vas_p)[,3]
vas_p$S2vH1_rate <- model.matrix(~rate, vas_p)[,4]
vas_p$F2vH1_rate <- model.matrix(~rate, vas_p)[,5]
vas_p$F3vH1_rate <- model.matrix(~rate, vas_p)[,6]
vas_p$F4vH1_rate <- model.matrix(~rate, vas_p)[,7]

# SITs: prop_wpm_5
model.matrix(~prop_wpm_5, sits_p) %>% head()
sits_p$S3vH1_prop <- model.matrix(~prop_wpm_5, sits_p)[,2]
sits_p$S2vH1_prop <- model.matrix(~prop_wpm_5, sits_p)[,3]
sits_p$F2vH1_prop <- model.matrix(~prop_wpm_5, sits_p)[,4]
sits_p$F3vH1_prop <- model.matrix(~prop_wpm_5, sits_p)[,5]

model.matrix(~rate, conv_p) %>% head()
conv_p$S4vH1_rate <- model.matrix(~rate, conv_p)[,2]
conv_p$S3vH1_rate <- model.matrix(~rate, conv_p)[,3]
conv_p$S2vH1_rate <- model.matrix(~rate, conv_p)[,4]
conv_p$F2vH1_rate <- model.matrix(~rate, conv_p)[,5]
conv_p$F3vH1_rate <- model.matrix(~rate, conv_p)[,6]
conv_p$F4vH1_rate <- model.matrix(~rate, conv_p)[,7]

# Ready for modelling (almost)----

## Sandbox Dataviz ----
summary(vas) # no NAs; good
summary(vas$participant)
# ERROR SPOTTED: participants 208 and 512 didn't make it to the SITs because i am an idiot adn something happened in creating the playlists. 
p = vas %>%
     ggplot(aes(x = group, y = percentage, fill = rate))+geom_boxplot()
p + facet_wrap(~listener) # same pattern across listeners
p + facet_grid(task~group, scales = "free")

# Effect of item? Look pretty constant for the most part
vas %>% ggplot(aes(x = item, y = percentage, fill = group))+geom_boxplot()+facet_wrap(~group)
vas %>% ggplot(aes(x = item, y = percentage, fill = rate))+geom_boxplot()+facet_wrap(~group)

# Error bar plots
vas_p %>% ggplot(aes(x = rate, y = percentage))+
     stat_summary(fun.data="mean_cl_boot", geom='errorbar',width=0.2, aes(color=group))+
     geom_jitter(alpha = 0.25, aes(color = group))+
     geom_line(aes(group = group, color = group), data = vas_gr)+
     geom_vline(xintercept = which(vas_p$rate=="h1"), linetype = "dashed")

# conv_p %>% ggplot(aes(x=rate, y = percentage))+
#      stat_summary(fun.data="mean_cl_boot", geom='errorbar',width=0.2, aes(color=group))+
#      geom_jitter(alpha = 0.25, aes(color = group))+
#      geom_line(aes(group = group, color = group), data = conv_gr)+
#      geom_vline(xintercept = which(conv_p$rate=="h1"), linetype = "dashed")

